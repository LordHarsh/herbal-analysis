{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'your_dataset.csv' is the name of your dataset CSV file\n",
    "dataset = pd.read_csv(r'Herbals and preperations.csv', encoding='latin-1')  # Change the separator if needed\n",
    "\n",
    "# Filter rows with disease 'Diabetes'\n",
    "diabetes_data = dataset[dataset['disease_category'] == 'Diabetes']\n",
    "\n",
    "# Remove specified columns\n",
    "columns_to_remove = ['author', 'herb_part', 'disease_category', 'taste', 'potency', 'ultimate_taste', 'inherent_action']\n",
    "diabetes_data_filtered = diabetes_data.drop(columns=columns_to_remove)\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "diabetes_data_filtered.to_csv('diabetes_filtered_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the separator if needed\n",
    "\n",
    "# Filter rows with disease 'Diabetes'\n",
    "diabetes_data = dataset[dataset['disease_category'] == 'Diabetes']\n",
    "\n",
    "# Group by common drugs and concatenate the 'bot_name' values\n",
    "grouped_data = diabetes_data.groupby('drug')['bot_name'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Save the result to a text file\n",
    "with open('output.txt', 'w') as f:\n",
    "    for index, row in grouped_data.iterrows():\n",
    "        f.write(row['bot_name'] + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "{'Bitter, Astringent, Pungent'}\n",
      "{'Sweet '}\n",
      "{'Hot'}\n",
      "{'Bitter, karakarapu'}\n",
      "{'Sweet, spicy'}\n",
      "{'Bitter'}\n",
      "{'Astringent, Bitter '}\n",
      "{'Bitter, Astringent '}\n",
      "{'mildBitter , Astringent'}\n",
      "{'sweet, Astringent '}\n",
      "{'Bitter , Pungent'}\n",
      "{'sweet, mildBitter '}\n",
      "{'Pungent , sweet'}\n",
      "{'pungent , Sweet'}\n",
      "{'Pungent , Sweet'}\n",
      "{'Pungent, Bitter'}\n",
      "{'Pungent '}\n",
      "{'Bitter , Viruvirupu'}\n",
      "{'Bitter , mildAstringent '}\n",
      "{'Astringent, sweet'}\n",
      "{'Astringent '}\n",
      "{'Astringent, viruvirupu '}\n",
      "{'Bitter , Arpapungent'}\n",
      "{'Bitter , Pungent '}\n",
      "{'sweet'}\n",
      "{'Sour, Astringent, sweet'}\n",
      "{'Bitter , Astringent'}\n",
      "{'Bitter(veguttal)'}\n",
      "{'Bitter (kumatal)'}\n",
      "{'Pungent'}\n",
      "{'Astringent , mildBitter '}\n",
      "{'pungent , Viruvirupu'}\n",
      "{'null'}\n",
      "{'Bitter , Astringent '}\n",
      "{'Bitter, Pungent '}\n",
      "{'Pungent, viruvirupu'}\n",
      "{'Cold '}\n",
      "{'sweet, Pungent'}\n",
      "{'Bitter, Astringent'}\n",
      "{'mildBitter, Pungent'}\n",
      "{'Sweet , Pungent'}\n",
      "{'Sour'}\n",
      "{'Astringent , Bitter'}\n",
      "{'Astringent'}\n",
      "{'pungent '}\n",
      "{'hot'}\n",
      "{'Astringent , Pungent'}\n",
      "{'sweet, mildBitter'}\n",
      "{'Sweet'}\n",
      "{'Spicy '}\n",
      "{'Bitter '}\n",
      "{'sweet '}\n",
      "{'Kasapu , virivirupu'}\n",
      "{'Bitter , pungent , Sweet'}\n",
      "{'Karakarapu'}\n"
     ]
    }
   ],
   "source": [
    "class Relim:\n",
    "    def __init__(self, min_support):\n",
    "        self.min_support = min_support\n",
    "        self.freq_itemsets = []\n",
    "\n",
    "    def run_relim(self, transactions, prefix=None):\n",
    "        if prefix is None:\n",
    "            prefix = set()\n",
    "\n",
    "        items = self.get_items(transactions)\n",
    "\n",
    "        for item in items:\n",
    "            new_prefix = prefix.copy()\n",
    "            new_prefix.add(item)\n",
    "\n",
    "            # Count the support of the new itemset\n",
    "            support = self.count_support(transactions, new_prefix)\n",
    "\n",
    "            if support >= self.min_support:\n",
    "                # Add the frequent itemset to the result\n",
    "                self.freq_itemsets.append(new_prefix)\n",
    "\n",
    "                # Generate conditional database for the next recursion\n",
    "                conditional_database = self.generate_conditional_database(transactions, new_prefix)\n",
    "\n",
    "                if conditional_database:\n",
    "                    # Recursively run the algorithm on the conditional database\n",
    "                    self.run_relim(conditional_database, new_prefix)\n",
    "\n",
    "    def get_items(self, transactions):\n",
    "        items = set()\n",
    "        for transaction in transactions:\n",
    "            items.update(transaction)\n",
    "        return items\n",
    "\n",
    "    def count_support(self, transactions, itemset):\n",
    "        count = 0\n",
    "        for transaction in transactions:\n",
    "            if itemset.issubset(transaction):\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def generate_conditional_database(self, transactions, itemset):\n",
    "        conditional_database = []\n",
    "        for transaction in transactions:\n",
    "            if itemset.issubset(transaction):\n",
    "                # Remove items in itemset from the transaction\n",
    "                new_transaction = transaction - itemset\n",
    "                conditional_database.append(new_transaction)\n",
    "        return conditional_database\n",
    "\n",
    "\n",
    "# Preprocess the dataset\n",
    "import csv\n",
    "\n",
    "dataset = []\n",
    "\n",
    "with open(r'initial-data/Herbals and preperations.csv', newline='', encoding='latin-1') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        # Consider specific columns as items in each transaction\n",
    "        transaction = set([row['taste'], row['potency'], row['ultimate_taste']])\n",
    "        dataset.append(transaction)\n",
    "\n",
    "# Set the minimum support threshold\n",
    "min_support = 2\n",
    "\n",
    "# Run the Relim algorithm\n",
    "relim = Relim(min_support)\n",
    "relim.run_relim(dataset)\n",
    "\n",
    "# Display frequent itemsets\n",
    "print(\"Frequent Itemsets:\")\n",
    "for itemset in relim.freq_itemsets:\n",
    "    print(itemset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m disease_categories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m---> 97\u001b[0m     disease_categories\u001b[38;5;241m.\u001b[39madd(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdisease_category\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Display frequent itemsets and their support for each disease\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m disease \u001b[38;5;129;01min\u001b[39;00m disease_categories:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'set' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "class Relim:\n",
    "    def __init__(self, min_support, min_confidence):\n",
    "        self.min_support = min_support\n",
    "        self.min_confidence = min_confidence\n",
    "        self.freq_itemsets = []\n",
    "        self.association_rules = []\n",
    "        self.transactions = []\n",
    "\n",
    "    def run_relim(self, transactions, prefix=None):\n",
    "        if prefix is None:\n",
    "            prefix = set()\n",
    "\n",
    "        items = self.get_items(transactions)\n",
    "\n",
    "        for item in items:\n",
    "            new_prefix = prefix.copy()\n",
    "            new_prefix.add(item)\n",
    "\n",
    "            # Count the support of the new itemset\n",
    "            support = self.count_support(transactions, new_prefix)\n",
    "\n",
    "            if support >= self.min_support:\n",
    "                # Add the frequent itemset to the result\n",
    "                self.freq_itemsets.append((new_prefix, support))\n",
    "\n",
    "                # Generate conditional database for the next recursion\n",
    "                conditional_database = self.generate_conditional_database(transactions, new_prefix)\n",
    "\n",
    "                if conditional_database:\n",
    "                    # Recursively run the algorithm on the conditional database\n",
    "                    self.run_relim(conditional_database, new_prefix)\n",
    "\n",
    "    def generate_association_rules(self):\n",
    "        for itemset, support in self.freq_itemsets:\n",
    "            if len(itemset) > 1:\n",
    "                self.generate_rules_from_itemset(itemset, support)\n",
    "\n",
    "    def generate_rules_from_itemset(self, itemset, support):\n",
    "        for i in range(1, len(itemset)):\n",
    "            antecedent = set(itemset[:i])\n",
    "            consequent = set(itemset[i:])\n",
    "\n",
    "            confidence = support / self.count_support(self.transactions, antecedent)\n",
    "\n",
    "            if confidence >= self.min_confidence:\n",
    "                rule = (antecedent, consequent, confidence)\n",
    "                self.association_rules.append(rule)\n",
    "\n",
    "    def get_items(self, transactions):\n",
    "        items = set()\n",
    "        for transaction in transactions:\n",
    "            items.update(transaction)\n",
    "        return items\n",
    "\n",
    "    def count_support(self, transactions, itemset):\n",
    "        count = 0\n",
    "        for transaction in transactions:\n",
    "            if itemset.issubset(transaction):\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def generate_conditional_database(self, transactions, itemset):\n",
    "        conditional_database = []\n",
    "        for transaction in transactions:\n",
    "            if itemset.issubset(transaction):\n",
    "                # Remove items in itemset from the transaction\n",
    "                new_transaction = transaction - itemset\n",
    "                conditional_database.append(new_transaction)\n",
    "        return conditional_database\n",
    "\n",
    "\n",
    "# Preprocess the dataset\n",
    "import csv\n",
    "\n",
    "dataset = []\n",
    "\n",
    "with open(r'initial-data/Herbals and preperations.csv', newline='', encoding='latin-1') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        # Consider specific columns as items in each transaction\n",
    "        transaction = set([row['taste'], row['potency'], row['ultimate_taste']])\n",
    "        dataset.append(transaction)\n",
    "\n",
    "# Set the minimum support and confidence thresholds\n",
    "min_support = 0.1\n",
    "min_confidence = 0.7\n",
    "\n",
    "# Run the Relim algorithm\n",
    "relim = Relim(min_support, min_confidence)\n",
    "relim.transactions = dataset\n",
    "relim.run_relim(dataset)\n",
    "relim.generate_association_rules()\n",
    "\n",
    "# Extract unique disease categories\n",
    "disease_categories = set()\n",
    "for row in dataset:\n",
    "    disease_categories.add(row['disease_category'])\n",
    "\n",
    "# Display frequent itemsets and their support for each disease\n",
    "for disease in disease_categories:\n",
    "    print(f\"\\nFrequent Itemsets for {disease}:\")\n",
    "    print(\",support,itemsets\")\n",
    "    index = 0\n",
    "    for itemset, support in relim.freq_itemsets:\n",
    "        if any(keyword.lower() in disease.lower() for keyword in itemset):\n",
    "            print(f\"{index},{support:.6f},{itemset}\")\n",
    "            index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\AppData\\Local\\Temp\\ipykernel_12144\\1472720866.py:25: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  transactions = transactions.applymap(lambda x: 1 if x > 0 else 0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m itemsets_drugs\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Apply Relim\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m frequent_itemset_relim \u001b[38;5;241m=\u001b[39m \u001b[43mapply_relim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dia_aga\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Display results for different group counts\u001b[39;00m\n\u001b[0;32m     44\u001b[0m display_relim_results(frequent_itemset_relim, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupport\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 29\u001b[0m, in \u001b[0;36mapply_relim\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     26\u001b[0m transaction_list \u001b[38;5;241m=\u001b[39m transactions\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Perform Relim\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m relim_results \u001b[38;5;241m=\u001b[39m \u001b[43mitemmining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransaction_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransaction_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Convert the result to a DataFrame\u001b[39;00m\n\u001b[0;32m     32\u001b[0m itemsets_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([(\u001b[38;5;28mfrozenset\u001b[39m(k), v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m relim_results\u001b[38;5;241m.\u001b[39mitems()], columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitemsets\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupport\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\envs\\tf\\lib\\site-packages\\pymining\\itemmining.py:205\u001b[0m, in \u001b[0;36mrelim\u001b[1;34m(rinput, min_support)\u001b[0m\n\u001b[0;32m    203\u001b[0m fis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    204\u001b[0m report \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 205\u001b[0m \u001b[43m_relim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrinput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_support\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m report\n",
      "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\envs\\tf\\lib\\site-packages\\pymining\\itemmining.py:210\u001b[0m, in \u001b[0;36m_relim\u001b[1;34m(rinput, fis, report, min_support)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_relim\u001b[39m(rinput, fis, report, min_support):\n\u001b[1;32m--> 210\u001b[0m     (relim_input, key_map) \u001b[38;5;241m=\u001b[39m rinput\n\u001b[0;32m    211\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    212\u001b[0m     a \u001b[38;5;241m=\u001b[39m relim_input\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from pymining import itemmining\n",
    "import pandas as pd\n",
    "data = pd.read_csv(r'initial-data/Herbals and preperations.csv', encoding='latin-1')\n",
    "data = data.apply(lambda x: x.str.lower() if x.dtype == \"object\" else x)\n",
    "data=data.dropna(subset=['bot_name'])\n",
    "data['bot_name'] = data['bot_name'].str.strip()\n",
    "data = data.join(pd.get_dummies(data['author'].str.lower().str.strip(), prefix='Author'))\n",
    "data['author'] = data['author'].dropna(axis=0).str.strip()\n",
    "data = data.apply(lambda x: x.str.lower() if x.dtype == \"object\" else x)\n",
    "data=data.dropna(subset=['bot_name'])\n",
    "data['bot_name'] = data['bot_name'].str.strip()\n",
    "data.loc[data.bot_name.isin(['terminalia chebula', 'terminalia bellarica', 'phyllanthus emblica']), 'bot_name'] = 'triphala'\n",
    "data.loc[data.bot_name.isin(['piper nigrum', 'piper longum', 'zingiber officinale']), 'bot_name'] = 'trikatu'\n",
    "data.loc[(data.disease_category == 'diabetes') & (data.bot_name == 'saccharum officinarum'), 'bot_name'] = 'tinospora cordifolia'\n",
    "data_dia = data.loc[data['disease_category'].apply(lambda x: x in ['diab/ tb ', 'diabetes'])].copy()\n",
    "data_tub =  data.loc[data['disease_category'].apply(lambda x: x in ['diab/ tb ', 'tuberculosis '])].copy()\n",
    "data_dia_aga = data_dia.loc[data_dia['author'].apply(lambda x: x in ['agathiyar'])]\n",
    "data_dia_the = data_dia.loc[data_dia['author'].apply(lambda x: x in ['therayar'])]\n",
    "data_tub_aga = data_tub.loc[data_tub['author'].apply(lambda x: x in ['agathiyar'])]\n",
    "data_tub_the = data_tub.loc[data_tub['author'].apply(lambda x: x in ['therayar'])]\n",
    "def apply_relim(df):\n",
    "    # Convert the dataset to the format expected by Relim\n",
    "    transactions = df.groupby(['drug', 'bot_name']).size().unstack(fill_value=0)\n",
    "    transactions = transactions.applymap(lambda x: 1 if x > 0 else 0)\n",
    "    transaction_list = transactions.values.tolist()\n",
    "\n",
    "    # Perform Relim\n",
    "    relim_results = itemmining.relim(transaction_list, (0.1, len(transaction_list)))\n",
    "\n",
    "    # Convert the result to a DataFrame\n",
    "    itemsets_df = pd.DataFrame([(frozenset(k), v) for k, v in relim_results.items()], columns=['itemsets', 'support'])\n",
    "    return itemsets_df\n",
    "\n",
    "def display_relim_results(itemsets_df, count=3, measure='support'):\n",
    "    itemsets_drugs = itemsets_df[itemsets_df['itemsets'].apply(lambda x: len(x) == count)]\n",
    "    itemsets_drugs = itemsets_drugs.sort_values(by=measure, ascending=False)\n",
    "    return itemsets_drugs\n",
    "\n",
    "# Apply Relim\n",
    "frequent_itemset_relim = apply_relim(data_dia_aga)\n",
    "\n",
    "# Display results for different group counts\n",
    "display_relim_results(frequent_itemset_relim, 3, 'support')\n",
    "display_relim_results(frequent_itemset_relim, 4, 'support')\n",
    "display_relim_results(frequent_itemset_relim, 5, 'support')\n",
    "\n",
    "# Note: Since pymining does not provide direct support for association rules, you may need to implement that part separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed Frequent Itemsets:\n",
      "                      itemsets   support\n",
      "0                      [sweet]  0.341176\n",
      "1                      [cold ]  0.335294\n",
      "2                    [unknown]  0.329412\n",
      "3                        [hot]  0.458824\n",
      "4                     [spicy ]  0.447059\n",
      "5                     [bitter]  0.111765\n",
      "6                    [legyam ]  0.205882\n",
      "7            [muyal kirutham ]  0.105882\n",
      "8         [kool panda legyam ]  0.141176\n",
      "9   [thalisapathiri choornam ]  0.152941\n",
      "10              [sweet, cold ]  0.288235\n",
      "11               [spicy , hot]  0.405882\n",
      "12       [spicy , bitter, hot]  0.105882\n",
      "13           [spicy , legyam ]  0.117647\n",
      "14              [legyam , hot]  0.105882\n",
      "15      [spicy , legyam , hot]  0.100000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(r'initial-data/Herbals and preperations.csv', encoding='latin-1')\n",
    "\n",
    "# Preprocess the data\n",
    "data = data.apply(lambda x: x.str.lower() if x.dtype == \"object\" else x)\n",
    "data = data.dropna(subset=['bot_name'])\n",
    "data['bot_name'] = data['bot_name'].str.strip()\n",
    "data = data.join(pd.get_dummies(data['author'].str.lower().str.strip(), prefix='Author'))\n",
    "data['author'] = data['author'].dropna(axis=0).str.strip()\n",
    "data = data.apply(lambda x: x.str.lower() if x.dtype == \"object\" else x)\n",
    "data = data.dropna(subset=['bot_name'])\n",
    "data['bot_name'] = data['bot_name'].str.strip()\n",
    "\n",
    "# Filter data for author \"Agathiyar\" and disease category \"Diabetes\"\n",
    "data_dia_aga = data[(data['author'] == 'agathiyar') & (data['disease_category'].str.lower().str.strip() == 'diabetes')]\n",
    "\n",
    "# Extract relevant columns for itemset mining\n",
    "columns_of_interest = ['drug', 'taste', 'potency', 'ultimate_taste', 'inherent_action']\n",
    "data_for_mining = data_dia_aga[columns_of_interest]\n",
    "\n",
    "# Convert NaN values to a placeholder (e.g., 'unknown')\n",
    "data_for_mining = data_for_mining.fillna('unknown')\n",
    "\n",
    "# Convert the dataset to the format expected by mlxtend\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(data_for_mining.values).transform(data_for_mining.values)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Task 1: Compute Frequent Item Set using mlxtend.frequent_patterns.fpgrowth\n",
    "frequent_itemsets = fpgrowth(df, min_support=0.1, use_colnames=True)\n",
    "\n",
    "# Task 2: Find Closed frequent itemset using frequent itemset found in Task 1\n",
    "def closed_frequent_itemsets(frequent_itemsets):\n",
    "    closed_itemsets = []\n",
    "    for index, row in frequent_itemsets.iterrows():\n",
    "        is_closed = True\n",
    "        itemset = row['itemsets']\n",
    "        support = row['support']\n",
    "        for _, other_row in frequent_itemsets.iterrows():\n",
    "            if index != _:\n",
    "                other_itemset = other_row['itemsets']\n",
    "                other_support = other_row['support']\n",
    "                if itemset.issubset(other_itemset) and support == other_support:\n",
    "                    is_closed = False\n",
    "                    break\n",
    "        if is_closed:\n",
    "            closed_itemsets.append({'itemsets': list(itemset), 'support': support})\n",
    "    return closed_itemsets\n",
    "\n",
    "# Task 3: Display the closed frequent itemsets\n",
    "closed_itemsets = closed_frequent_itemsets(frequent_itemsets)\n",
    "closed_df = pd.DataFrame(closed_itemsets)\n",
    "print('Closed Frequent Itemsets:')\n",
    "print(closed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'association_rules_outputs\\closed_itemsets_agathiyar_diab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 70\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Save the closed frequent itemsets to CSV\u001b[39;00m\n\u001b[0;32m     69\u001b[0m closed_csv_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosed_itemsets_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauthor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisease_category\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 70\u001b[0m \u001b[43mclosed_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosed_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Task 4: Generate Association Rules\u001b[39;00m\n\u001b[0;32m     73\u001b[0m rules \u001b[38;5;241m=\u001b[39m association_rules(frequent_itemsets, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlift\u001b[39m\u001b[38;5;124m'\u001b[39m, min_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3891\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3893\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3894\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3895\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3899\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3900\u001b[0m )\n\u001b[1;32m-> 3902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3905\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3907\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1134\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1135\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1151\u001b[0m )\n\u001b[1;32m-> 1152\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:247\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    257\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    258\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\common.py:739\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 739\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\common.py:604\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    602\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'association_rules_outputs\\closed_itemsets_agathiyar_diab'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(r'initial-data/Herbals and preperations.csv', encoding='latin-1')\n",
    "\n",
    "# Preprocess the data\n",
    "data = data.apply(lambda x: x.str.lower() if x.dtype == \"object\" else x)\n",
    "data = data.dropna(subset=['bot_name'])\n",
    "data['bot_name'] = data['bot_name'].str.strip()\n",
    "data = data.join(pd.get_dummies(data['author'].str.lower().str.strip(), prefix='Author'))\n",
    "data['author'] = data['author'].dropna(axis=0).str.strip()\n",
    "data = data.apply(lambda x: x.str.lower() if x.dtype == \"object\" else x)\n",
    "data = data.dropna(subset=['bot_name'])\n",
    "data['bot_name'] = data['bot_name'].str.strip()\n",
    "\n",
    "# Create a directory to store outputs\n",
    "output_dir = 'association_rules_outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define categories based on 'author' and 'disease_category'\n",
    "categories = data.groupby(['author', 'disease_category'])\n",
    "\n",
    "# Process each category\n",
    "for category_name, category_data in categories:\n",
    "    author, disease_category = category_name\n",
    "\n",
    "    # Extract relevant columns for itemset mining\n",
    "    columns_of_interest = ['drug', 'taste', 'potency', 'ultimate_taste', 'inherent_action']\n",
    "    data_for_mining = category_data[columns_of_interest]\n",
    "\n",
    "    # Convert NaN values to a placeholder (e.g., 'unknown')\n",
    "    data_for_mining = data_for_mining.fillna('unknown')\n",
    "\n",
    "    # Convert the dataset to the format expected by mlxtend\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(data_for_mining.values).transform(data_for_mining.values)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "    # Task 1: Compute Frequent Item Set using mlxtend.frequent_patterns.fpgrowth\n",
    "    frequent_itemsets = fpgrowth(df, min_support=0.1, use_colnames=True)\n",
    "\n",
    "    # Task 2: Find Closed frequent itemset using frequent itemset found in Task 1\n",
    "    def closed_frequent_itemsets(frequent_itemsets):\n",
    "        closed_itemsets = []\n",
    "        for index, row in frequent_itemsets.iterrows():\n",
    "            is_closed = True\n",
    "            itemset = row['itemsets']\n",
    "            support = row['support']\n",
    "            for _, other_row in frequent_itemsets.iterrows():\n",
    "                if index != _:\n",
    "                    other_itemset = other_row['itemsets']\n",
    "                    other_support = other_row['support']\n",
    "                    if itemset.issubset(other_itemset) and support == other_support:\n",
    "                        is_closed = False\n",
    "                        break\n",
    "            if is_closed:\n",
    "                closed_itemsets.append({'itemsets': list(itemset), 'support': support})\n",
    "        return closed_itemsets\n",
    "\n",
    "    # Task 3: Display the closed frequent itemsets\n",
    "    closed_itemsets = closed_frequent_itemsets(frequent_itemsets)\n",
    "    closed_df = pd.DataFrame(closed_itemsets)\n",
    "\n",
    "    # Save the closed frequent itemsets to CSV\n",
    "    closed_csv_path = os.path.join(output_dir, f'closed_itemsets_{author}_{disease_category}.csv')\n",
    "    closed_df.to_csv(closed_csv_path, index=False)\n",
    "\n",
    "    # Task 4: Generate Association Rules\n",
    "    rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1.0)\n",
    "\n",
    "    # Save the association rules to CSV\n",
    "    rules_csv_path = os.path.join(output_dir, f'association_rules_{author}_{disease_category}.csv')\n",
    "    rules.to_csv(rules_csv_path, index=False)\n",
    "\n",
    "    # Task 5: Generate and Save Graphs\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot Support vs. Itemsets\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.barh(range(len(frequent_itemsets)), frequent_itemsets['support'], align='center')\n",
    "    plt.yticks(range(len(frequent_itemsets)), frequent_itemsets['itemsets'])\n",
    "    plt.xlabel('Support')\n",
    "    plt.title('Support vs. Itemsets')\n",
    "\n",
    "    # Plot Confidence vs. Lift for Association Rules\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(rules['confidence'], rules['lift'])\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Lift')\n",
    "    plt.title('Confidence vs. Lift for Association Rules')\n",
    "\n",
    "    # Save the graph\n",
    "    graph_path = os.path.join(output_dir, f'association_rules_graph_{author}_{disease_category}.png')\n",
    "    plt.savefig(graph_path)\n",
    "    plt.close()\n",
    "\n",
    "print('Processing completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'association_rules_outputs\\closed_itemsets_agathiyar_diab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Save the closed frequent itemsets to CSV\u001b[39;00m\n\u001b[0;32m     71\u001b[0m closed_csv_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosed_itemsets_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauthor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisease_category\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m \u001b[43mclosed_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosed_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Task 4: Generate Association Rules\u001b[39;00m\n\u001b[0;32m     75\u001b[0m rules \u001b[38;5;241m=\u001b[39m association_rules(frequent_itemsets, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlift\u001b[39m\u001b[38;5;124m'\u001b[39m, min_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3891\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3893\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3894\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3895\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3899\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3900\u001b[0m )\n\u001b[1;32m-> 3902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3905\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3907\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1134\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1135\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1151\u001b[0m )\n\u001b[1;32m-> 1152\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:247\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    257\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    258\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\common.py:739\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 739\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\common.py:604\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    602\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'association_rules_outputs\\closed_itemsets_agathiyar_diab'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('Herbals and preperations.csv', encoding='latin-1')\n",
    "\n",
    "# Preprocess the data\n",
    "data = data.apply(lambda x: x.str.lower() if x.dtype == \"object\" else x)\n",
    "data = data.dropna(subset=['bot_name'])\n",
    "data['bot_name'] = data['bot_name'].str.strip()\n",
    "data = data.join(pd.get_dummies(data['author'].str.lower().str.strip(), prefix='Author'))\n",
    "data['author'] = data['author'].dropna(axis=0).str.strip()\n",
    "data = data.apply(lambda x: x.str.lower() if x.dtype == \"object\" else x)\n",
    "data = data.dropna(subset=['bot_name'])\n",
    "data['bot_name'] = data['bot_name'].str.strip()\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = 'association_rules_outputs'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define categories based on 'author' and 'disease_category'\n",
    "categories = data.groupby(['author', 'disease_category'])\n",
    "\n",
    "# Process each category\n",
    "for category_name, category_data in categories:\n",
    "    author, disease_category = category_name\n",
    "\n",
    "    # Extract relevant columns for itemset mining\n",
    "    columns_of_interest = ['drug', 'taste', 'potency', 'ultimate_taste', 'inherent_action']\n",
    "    data_for_mining = category_data[columns_of_interest]\n",
    "\n",
    "    # Convert NaN values to a placeholder (e.g., 'unknown')\n",
    "    data_for_mining = data_for_mining.fillna('unknown')\n",
    "\n",
    "    # Convert the dataset to the format expected by mlxtend\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(data_for_mining.values).transform(data_for_mining.values)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "    # Task 1: Compute Frequent Item Set using mlxtend.frequent_patterns.fpgrowth\n",
    "    frequent_itemsets = fpgrowth(df, min_support=0.1, use_colnames=True)\n",
    "\n",
    "    # Task 2: Find Closed frequent itemset using frequent itemset found in Task 1\n",
    "    def closed_frequent_itemsets(frequent_itemsets):\n",
    "        closed_itemsets = []\n",
    "        for index, row in frequent_itemsets.iterrows():\n",
    "            is_closed = True\n",
    "            itemset = row['itemsets']\n",
    "            support = row['support']\n",
    "            for _, other_row in frequent_itemsets.iterrows():\n",
    "                if index != _:\n",
    "                    other_itemset = other_row['itemsets']\n",
    "                    other_support = other_row['support']\n",
    "                    if itemset.issubset(other_itemset) and support == other_support:\n",
    "                        is_closed = False\n",
    "                        break\n",
    "            if is_closed:\n",
    "                closed_itemsets.append({'itemsets': list(itemset), 'support': support})\n",
    "        return closed_itemsets\n",
    "\n",
    "    # Task 3: Display the closed frequent itemsets\n",
    "    closed_itemsets = closed_frequent_itemsets(frequent_itemsets)\n",
    "    closed_df = pd.DataFrame(closed_itemsets)\n",
    "\n",
    "    # Save the closed frequent itemsets to CSV\n",
    "    closed_csv_path = os.path.join(output_dir, f'closed_itemsets_{author}_{disease_category}.csv')\n",
    "    closed_df.to_csv(closed_csv_path, index=False)\n",
    "\n",
    "    # Task 4: Generate Association Rules\n",
    "    rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1.0)\n",
    "\n",
    "    # Save the association rules to CSV\n",
    "    rules_csv_path = os.path.join(output_dir, f'association_rules_{author}_{disease_category}.csv')\n",
    "    rules.to_csv(rules_csv_path, index=False)\n",
    "\n",
    "    # Task 5: Generate and Save Graphs\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot Support vs. Itemsets\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.barh(range(len(frequent_itemsets)), frequent_itemsets['support'], align='center')\n",
    "    plt.yticks(range(len(frequent_itemsets)), frequent_itemsets['itemsets'])\n",
    "    plt.xlabel('Support')\n",
    "    plt.title('Support vs. Itemsets')\n",
    "\n",
    "    # Plot Confidence vs. Lift for Association Rules\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(rules['confidence'], rules['lift'])\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Lift')\n",
    "    plt.title('Confidence vs. Lift for Association Rules')\n",
    "\n",
    "    # Save the graph\n",
    "    graph_path = os.path.join(output_dir, f'association_rules_graph_{author}_{disease_category}.png')\n",
    "    plt.savefig(graph_path)\n",
    "    plt.close()\n",
    "\n",
    "print('Processing completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
